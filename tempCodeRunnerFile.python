# %%
import numpy as np 
import pandas as pd 
import csv 
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier 
from sklearn.decomposition import PCA 
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report, precision_score, confusion_matrix, ConfusionMatrixDisplay

# %% [markdown]
# Step 1: Preprocessing the data 

# %%
df = pd.read_csv('data.csv')
df.shape

# %%
df.head()

# %%
df['diagnosis'].value_counts()

# %%
df.info()

# %% [markdown]
# Replacing elements in the diagnosis column 
# 
# Benign (B) = 0;
# Malignant (M) = 1

# %%
# Replace 'M' with 1 and 'B' with 0 in the diagnosis column
df['diagnosis'] = df['diagnosis'].replace({'M': 1, 'B': 0})
print(df['diagnosis'].value_counts())

# %%
df.head()

# %%
# Displaying all the column names 
print("Names of all the columns")
for idx, col in enumerate(df.columns):
    print(f'{idx}. {col}')

# %%
# Create a list of columns to drop
columns_to_drop = []

# Check for columns to drop
if 'id' in df.columns:
    columns_to_drop.append('id')
    
if 'Unnamed: 32' in df.columns:
    columns_to_drop.append('Unnamed: 32')
    
# Drop columns if any were found
if columns_to_drop:
    df = df.drop(columns_to_drop, axis=1)
    print(f"\nDropped these columns: {columns_to_drop}")
else:
    print("\nNo columns were dropped")
    

df.head()

# %%
scaler = StandardScaler()

X = df.drop(['diagnosis'], axis = 1)
y = df['diagnosis']

X_scaled = scaler.fit_transform(X)

X_scaled_df = pd.DataFrame(X_scaled, columns = X.columns)

print("Original features before scaling")
display(X.head())

print("Original features after scaling")
display(X_scaled_df.head())

X_scaled_df.describe()

# %% [markdown]
# Step 2 : Exploratory Data Analysis (EDA)

# %% [markdown]
# # Exploratory Data Analysis (EDA)
# 
# We'll analyze the following aspects of our data:
# 
# 1. **Class Distribution**: Check if our dataset is balanced between benign and malignant cases
# 2. **Correlation Analysis**: Identify which features are highly correlated with each other
# 3. **Outlier Detection**: Look for any unusual values in our features using box plots
# 4. **Feature Distributions**: Examine how features are distributed across different diagnoses
# 
# This will help us:
# - Identify if we need to handle class imbalance
# - Select the most important features
# - Detect potential data quality issues
# - Understand the relationships between features

# %%
features_mean = [col for col in df.columns if '_mean' in col]

# Get statistical summary of all mean features
print("Statistical summary of mean features:")
print(df[features_mean].describe().round(2))

print("\nMean Features in the dataset:")
for idx, col in enumerate(features_mean, 1):
    print(f"{idx}. {col}")


# If you want to see value counts for each mean feature separately:
print("\nValue counts for each mean feature:")
for feature in features_mean:
    print("\nfeature")
    
    print(pd.cut(df[feature], bins = 5).value_counts().sort_index().round(3))


# %%
plt.figure(figsize = (15, 8))
demo_features = features_mean[:4]

for idx, feature in enumerate(demo_features, 1):
    plt.subplot(2, 2, idx)
    sns.histplot(data = df, x = feature, hue = 'diagnosis', bins = 20, alpha = 0.6)
    plt.title(f'Distribution of {feature}\nby Diagnosis')
    plt.xlabel(feature)
    plt.ylabel('count') 
    plt.legend(title = 'Diagnosis', labels = ['Benign', 'Malignant'])

plt.tight_layout()
plt.show()

# %%
feature = 'radius_mean'

plt.figure(figsize=(15, 5))

# Original data
plt.subplot(1, 3, 1)
sns.scatterplot(data=df, x=feature, y='diagnosis')
plt.title('Original Data\n(Each Point is a Tumor)')
plt.ylabel('Diagnosis (0=Benign, 1=Malignant)')

# Binned data
plt.subplot(1, 3, 2)
binned_data = pd.cut(df[feature], bins=5)
sns.countplot(x=binned_data, hue=df['diagnosis'])
plt.title('Binned Data\n(Counts per Bin)')
plt.xticks(rotation=45)
plt.xlabel('Radius Mean Ranges')
plt.legend(title='Diagnosis', labels=['Benign', 'Malignant'])

# Risk analysis
plt.subplot(1, 3, 3)
bin_proportions = df.groupby(binned_data)['diagnosis'].mean() * 100
bin_proportions.plot(kind='bar')
plt.title('Risk Analysis\n(% Malignant per Bin)')
plt.ylabel('Percentage Malignant')
plt.xlabel('Radius Mean Ranges')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Print risk analysis
print("\nRisk Analysis for each radius_mean range:")
for bin_range, percentage in bin_proportions.items():
    print(f"{bin_range}: {percentage:.1f}% malignant")

# %%
# Checking correlations 
plt.figure(figsize = (12, 8))
corr = df.corr()
sns.heatmap(corr, cmap = 'coolwarm', center = 0)
plt.title('Feature Correlation Heatmap')
plt.show()

# %%
# Ensure clean labels
df['diagnosis_label'] = df['diagnosis'].map({0: 'Benign', 1: 'Malignant'})

plt.figure(figsize=(6, 4))
sns.countplot(
    data=df,
    x='diagnosis_label',
    hue='diagnosis_label',                      # <- required so palette is respected
    hue_order=['Benign', 'Malignant'],          # <- fixes order
    palette={'Benign': 'blue', 'Malignant': 'orange'},
    dodge=False,                                # 1 bar per category
    legend=False
)
plt.title('Class Distribution (Benign vs Malignant)')
plt.xlabel('Diagnosis')
plt.ylabel('Count')
plt.show()

print(df['diagnosis_label'].value_counts(normalize=True))

# %%
plt.figure(figsize = (12, 6))
sns.boxplot(x = 'diagnosis', y = 'radius_mean', data = df, hue = 'diagnosis', palette = {0 : 'orange', 1 : 'blue'}, legend = False)
plt.title('Radius Mean by Diagnosis')
plt.show()

# %% [markdown]
# Step 3 : Feature Selection/Engineering

# %%
# 1) numeric-only frame
df_num = df.select_dtypes(include=[np.number])

# 2) correlation matrix
corr = df_num.corr(numeric_only=True)

# 3) keep upper triangle to avoid duplicates
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))

# 4) list highly correlated pairs (|r| > 0.90)
high_pairs = (
    upper.abs()                           # use absolute correlation
         .stack()                         # Series indexed by (row, col)
         .rename_axis(['feature_1','feature_2'])
         .reset_index(name='corr')        # <- name is a string
         .query('corr > 0.90')
         .sort_values('corr', ascending=False)
)
print(high_pairs)


# %%
to_drop = set(high_pairs['feature_2'])
df_reduced = df_num.drop(columns = list(to_drop))
print(f"Dropping {len(to_drop)} columns: ", sorted(to_drop))
print("Remaining: ", df_reduced.columns.tolist())

# %%
X = df.select_dtypes(include = [np.number]).drop(['diagnosis'], axis = 1)
y = df['diagnosis']

selector = SelectKBest(score_func = f_classif, k = 10)
X_new = selector.fit_transform(X, y)

selected_features = X.columns[selector.get_support()]
print("Top 10 features : ", selected_features)

# %%
# Importing the RandomForestClassifier 

model = RandomForestClassifier(random_state = 42)
model.fit(X, y)

importances = pd.Series(model.feature_importances_, index = X.columns)
importances.nlargest(10).plot(kind='barh')
plt.title("Top 10 Important Features (Random Forest)")
plt.show()

# %%
pca = PCA(n_components = 10)
X_pca = pca.fit_transform(X)
print("Explained variance by 10 components: ", sum(pca.explained_variance_ratio_))

# %%
X = df_num.drop(columns = ['diagnosis'])
y = df_num['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)

X_scaled_train = scaler.fit_transform(X_train) 
X_scaled_test = scaler.transform(X_test) 

X_train_scaled = pd.DataFrame(X_scaled_train, columns = X.columns, index = X_train.index)
X_test_scaled = pd.DataFrame(X_scaled_test, columns = X.columns, index = X_test.index)

print("Train set size: ", X_scaled_train.shape[0])
print("Test set size: ", X_scaled_test.shape[0])

print("\nClass distribution in train set:", y_train.value_counts(normalize = True))
print("\nClass distribution in test set:", y_test.value_counts(normalize = True))

# %% [markdown]
# Training the models

# %% [markdown]
# Logistics Regression Model 

# %%
log_reg = LogisticRegression(max_iter = 1000, random_state = 42)
log_reg.fit(X_train_scaled, y_train)

y_predict_log = log_reg.predict(X_test_scaled)

print("Logistic Regression Scores:")
print(f"Accuracy: {accuracy_score(y_test, y_predict_log) * 100:.3f}")
print(f"Precision: {precision_score(y_test, y_predict_log) * 100:.3f}")
print(f"Recall: {recall_score(y_test, y_predict_log) * 100:.3f}")
print(f"F1 Score: {f1_score(y_test, y_predict_log) * 100:.3f}")

# %% [markdown]
# Random Forest Model 

# %%
rf = RandomForestClassifier(n_estimators = 100, random_state = 42)
rf.fit(X_train_scaled, y_train)

y_predict_rf = rf.predict(X_test_scaled)

print("Random Forest Scores:")
print(f"Accuracy: {accuracy_score(y_test, y_predict_rf) * 100:.3f}")
print(f"Precision: {precision_score(y_test, y_predict_rf) * 100:.3f}")
print(f"Recall: {recall_score(y_test, y_predict_rf) * 100:.3f}")
print(f"F1 Score: {f1_score(y_test, y_predict_rf) * 100:.3f}")

# %% [markdown]
# Support Vector Machine 

# %%
svm_model = SVC(kernel = 'rbf', random_state = 42)
svm_model.fit(X_train_scaled, y_train)

y_predict_svm_model = svm_model.predict(X_test_scaled)

print("Support Vector Machine Scores:")
print(f"Accuracy: {accuracy_score(y_test, y_predict_svm_model) * 100:.3f}")
print(f"Precision: {precision_score(y_test, y_predict_svm_model) * 100:.3f}")
print(f"Recall: {recall_score(y_test, y_predict_svm_model) * 100:.3f}")
print(f"F1 Score: {f1_score(y_test, y_predict_svm_model) * 100:.3f}")

# %% [markdown]
# Model comparison in tabular format 

# %%
results = []

results.append([
    "Logistics Regression",
    accuracy_score(y_test, y_predict_log) * 100,
    precision_score(y_test, y_predict_log) * 100,
    recall_score(y_test, y_predict_log) * 100,
    f1_score(y_test, y_predict_log) * 100
])

results.append([
    "Random Forest",
    accuracy_score(y_test, y_predict_rf) * 100,
    precision_score(y_test, y_predict_rf) * 100,
    recall_score(y_test, y_predict_rf) * 100,
    f1_score(y_test, y_predict_rf) * 100
])

results.append([
    "SVM",
    accuracy_score(y_test, y_predict_svm_model) * 100,
    precision_score(y_test, y_predict_svm_model) * 100,
    recall_score(y_test, y_predict_svm_model) * 100,
    f1_score(y_test, y_predict_svm_model) * 100
])

results_df = pd.DataFrame(
    results,
    columns = ['Model', 'Accuracy(%)', 'Precision(%)', 'Recall(%)', 'F1 Score(%)']
)

print("Model Comparison")
print(results_df.round(3).sort_index())

# %%
import matplotlib.pyplot as plt

# Set 'Model' as index so metrics become columns
results_df_plot = results_df.set_index("Model")

# Explicitly use matplotlib .subplots()
fig, ax = plt.subplots(figsize=(10,6))
results_df_plot.plot(kind="bar", ax=ax)

# Titles and labels
ax.set_title("Model Performance Comparison")
ax.set_ylabel("Score (%)")
ax.set_xlabel("Models")
ax.set_xticklabels(ax.get_xticklabels(), rotation=0)
ax.set_ylim(0, 105)
ax.legend(title="Metrics")

# Add value labels on top of bars
for container in ax.containers:
    ax.bar_label(container, fmt="%.1f", label_type="edge", padding=2)

plt.show()


# %% [markdown]
# Confusion Matrix

# %%
cm = confusion_matrix(y_test, y_predict_rf)
disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['Benign', 'Malignant'])
disp.plot(cmap = 'Blues')
plt.title("Confusion Matrix : Random Forest")
plt.show()

# %%
for n in [50, 100, 200]:
    rf = RandomForestClassifier(n_estimators=n, random_state=42)
    rf.fit(X_train_scaled, y_train)
    y_pred = rf.predict(X_test_scaled)
    print(f"n_estimators={n}, Accuracy={accuracy_score(y_test, y_pred)*100:.2f}%")


# %% [markdown]
# Printing the classification report 

# %%
print("Random Forest Report:")
print(classification_report(y_test, y_predict_rf, target_names = ['Benign', 'Malignant']))

# %%



